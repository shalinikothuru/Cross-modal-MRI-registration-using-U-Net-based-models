{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-12-10 17:53:35.738542: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-10 17:53:35.758455: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 17:53:35.758478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 17:53:35.759051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 17:53:35.762588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 17:53:36.177039: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dipy.viz import regtools\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from dipy.io.image import load_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 17:53:36.529329: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.548863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.548955: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.550588: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.550660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.550703: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.602626: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.602723: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.602784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-10 17:53:36.602827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46717 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        tf.print(e)\n",
    "\n",
    "# This is telling the default strategy for multi-GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# setting batch size. 1 per each gpu in this case.\n",
    "BATCH_SIZE_PER_REPLICA = 1\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross correlation loss for same modality registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncc(y_true, y_pred):\n",
    "    eps = tf.constant(1e-7, 'float32')\n",
    "    ndim = len(tf.keras.backend.int_shape(y_true))\n",
    "\n",
    "    y_true_mean = tf.reduce_mean(y_true, axis=range(1, ndim-1),\n",
    "                                  keepdims=True)\n",
    "    y_pred_mean = tf.reduce_mean(y_pred, axis=range(1, ndim-1),\n",
    "                                  keepdims=True)\n",
    "\n",
    "    y_true_std = tf.math.reduce_std(y_true, axis=range(1, ndim-1),\n",
    "                                    keepdims=True)\n",
    "    y_pred_std = tf.math.reduce_std(y_pred, axis=range(1, ndim-1),\n",
    "                                    keepdims=True)\n",
    "\n",
    "    y_true_hat = (y_true - y_true_mean) / (y_true_std + eps)\n",
    "    y_pred_hat = (y_pred - y_pred_mean) / (y_pred_std + eps)\n",
    "\n",
    "    return -tf.reduce_mean(y_true_hat * y_pred_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInformation(bin_centers,\n",
    "                      sigma_ratio=0.5,    # soft binning\n",
    "                      max_clip=1,\n",
    "                      crop_background=False, \n",
    "                      local_mi=False,\n",
    "                      patch_size=1):\n",
    "    \n",
    "    if local_mi:\n",
    "        return localMutualInformation(bin_centers, vol_size=(128, 128, 128), sigma_ratio=sigma_ratio, max_clip=max_clip, patch_size=patch_size)\n",
    "\n",
    "    else:\n",
    "        return globalMutualInformation(bin_centers, sigma_ratio, max_clip, crop_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localMutualInformation(bin_centers,\n",
    "                      vol_size=(128, 128, 128),\n",
    "                      sigma_ratio=0.5,\n",
    "                      max_clip=1,\n",
    "                      patch_size=1):\n",
    "\n",
    "    vol_bin_centers = K.variable(bin_centers)\n",
    "    num_bins = len(bin_centers)\n",
    "    sigma = np.mean(np.diff(bin_centers))*sigma_ratio\n",
    "\n",
    "    preterm = K.variable(1 / (2 * np.square(sigma)))\n",
    "\n",
    "    def local_mi(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, 0, max_clip)\n",
    "        y_true = K.clip(y_true, 0, max_clip)\n",
    "\n",
    "        on = [1, 1, 1, 1, num_bins]\n",
    "        vbc = K.reshape(vol_bin_centers, on)\n",
    "        \n",
    "        # compute padding sizes\n",
    "        x, y, z = vol_size\n",
    "        x_r = -x % patch_size\n",
    "        y_r = -y % patch_size\n",
    "        z_r = -z % patch_size\n",
    "        pad_dims = [[0,0]]\n",
    "        pad_dims.append([x_r//2, x_r - x_r//2])\n",
    "        pad_dims.append([y_r//2, y_r - y_r//2])\n",
    "        pad_dims.append([z_r//2, z_r - z_r//2])\n",
    "        pad_dims.append([0,0])\n",
    "        padding = tf.constant(pad_dims)\n",
    "\n",
    "        I_a = K.exp(- preterm * K.square(tf.pad(y_true, padding, 'CONSTANT')  - vbc))\n",
    "        I_a /= K.sum(I_a, -1, keepdims=True)\n",
    "\n",
    "        I_b = K.exp(- preterm * K.square(tf.pad(y_pred, padding, 'CONSTANT')  - vbc))\n",
    "        I_b /= K.sum(I_b, -1, keepdims=True)\n",
    "\n",
    "        I_a_patch = tf.reshape(I_a, [(x+x_r)//patch_size, patch_size, (y+y_r)//patch_size, patch_size, (z+z_r)//patch_size, patch_size, num_bins])\n",
    "        I_a_patch = tf.transpose(I_a_patch, [0, 2, 4, 1, 3, 5, 6])\n",
    "        I_a_patch = tf.reshape(I_a_patch, [-1, patch_size**3, num_bins])\n",
    "\n",
    "        I_b_patch = tf.reshape(I_b, [(x+x_r)//patch_size, patch_size, (y+y_r)//patch_size, patch_size, (z+z_r)//patch_size, patch_size, num_bins])\n",
    "        I_b_patch = tf.transpose(I_b_patch, [0, 2, 4, 1, 3, 5, 6])\n",
    "        I_b_patch = tf.reshape(I_b_patch, [-1, patch_size**3, num_bins])\n",
    "\n",
    "        I_a_permute = K.permute_dimensions(I_a_patch, (0,2,1))\n",
    "        pab = K.batch_dot(I_a_permute, I_b_patch) \n",
    "        pab /= patch_size**3\n",
    "        pa = tf.reduce_mean(I_a_patch, 1, keepdims=True)\n",
    "        pb = tf.reduce_mean(I_b_patch, 1, keepdims=True)\n",
    "        \n",
    "        papb = K.batch_dot(K.permute_dimensions(pa, (0,2,1)), pb) + K.epsilon()\n",
    "        mi = K.mean(K.sum(K.sum(pab * K.log(pab/papb + K.epsilon()), 1), 1))\n",
    "\n",
    "        return mi\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        return -local_mi(y_true, y_pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def globalMutualInformation(bin_centers,\n",
    "                      sigma_ratio=0.5,\n",
    "                      max_clip=1,\n",
    "                      crop_background=False):\n",
    "\n",
    "    vol_bin_centers = K.variable(bin_centers)\n",
    "    num_bins = len(bin_centers)\n",
    "    sigma = np.mean(np.diff(bin_centers))*sigma_ratio\n",
    "\n",
    "    preterm = K.variable(1 / (2 * np.square(sigma)))\n",
    "\n",
    "    def mi(y_true, y_pred):\n",
    "\n",
    "        y_pred = K.clip(y_pred, 0, max_clip)\n",
    "        y_true = K.clip(y_true, 0, max_clip)\n",
    "\n",
    "        if crop_background:\n",
    "            thresh = 0.0001\n",
    "            padding_size = 20\n",
    "            filt = tf.ones([padding_size, padding_size, padding_size, 1, 1])\n",
    "\n",
    "            smooth = tf.nn.conv3d(y_true, filt, [1, 1, 1, 1, 1], \"SAME\")\n",
    "            mask = smooth > thresh\n",
    "            y_pred = tf.boolean_mask(y_pred, mask)\n",
    "            y_true = tf.boolean_mask(y_true, mask)\n",
    "            y_pred = K.expand_dims(K.expand_dims(y_pred, 0), 2)\n",
    "            y_true = K.expand_dims(K.expand_dims(y_true, 0), 2)\n",
    "\n",
    "        else:\n",
    "            y_true = K.reshape(y_true, (-1, K.prod(K.shape(y_true)[1:])))\n",
    "            y_true = K.expand_dims(y_true, 2)\n",
    "            y_pred = K.reshape(y_pred, (-1, K.prod(K.shape(y_pred)[1:])))\n",
    "            y_pred = K.expand_dims(y_pred, 2)\n",
    "        \n",
    "        nb_voxels = tf.cast(K.shape(y_pred)[1], tf.float32)\n",
    "\n",
    "        o = [1, 1, np.prod(vol_bin_centers.get_shape().as_list())]\n",
    "        vbc = K.reshape(vol_bin_centers, o)\n",
    "\n",
    "        I_a = K.exp(- preterm * K.square(y_true  - vbc))\n",
    "        I_a /= K.sum(I_a, -1, keepdims=True)\n",
    "\n",
    "        I_b = K.exp(- preterm * K.square(y_pred  - vbc))\n",
    "        I_b /= K.sum(I_b, -1, keepdims=True)\n",
    "\n",
    "        I_a_permute = K.permute_dimensions(I_a, (0,2,1))\n",
    "        pab = K.batch_dot(I_a_permute, I_b)\n",
    "        pab /= nb_voxels\n",
    "        pa = tf.reduce_mean(I_a, 1, keepdims=True)\n",
    "        pb = tf.reduce_mean(I_b, 1, keepdims=True)\n",
    "        \n",
    "        papb = K.batch_dot(K.permute_dimensions(pa, (0,2,1)), pb) + K.epsilon()\n",
    "        mi = K.sum(K.sum(pab * K.log(pab/papb + K.epsilon()), 1), 1)\n",
    "\n",
    "        return mi\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        return -mi(y_true, y_pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_loss(phi, norm=2):\n",
    "    di = tf.abs(phi[:, 1:, :, :, :] - phi[:, :-1, :, :, :])\n",
    "    dj = tf.abs(phi[:, :, 1:, :, :] - phi[:, :, :-1, :, :])\n",
    "    dk = tf.abs(phi[:, :, :, 1:, :] - phi[:, :, :, :-1, :])\n",
    "\n",
    "    loss = tf.reduce_mean(di) + tf.reduce_mean(dj) + tf.reduce_mean(dk)\n",
    "    if norm == 2:\n",
    "        loss = tf.reduce_mean(di**2) + tf.reduce_mean(dj**2) + tf.reduce_mean(dk**2)    \n",
    "    return loss\n",
    "\n",
    "def regular_grid_3d(depth, height, width):\n",
    "    i = tf.linspace(-1.0, 1.0, depth)\n",
    "    j = tf.linspace(-1.0, 1.0, height)\n",
    "    k = tf.linspace(-1.0, 1.0, width)\n",
    "\n",
    "    I, J, K = tf.meshgrid(i, j, k, indexing='ij')\n",
    "\n",
    "    grid = tf.stack([I, J, K], axis=-1)\n",
    "    return grid\n",
    "\n",
    "def grid_sample_3d(moving, grid):\n",
    "    nb, nd, nh, nw, nc = tf.shape(moving)\n",
    "\n",
    "    i = grid[..., 0]  # shape (N, D, H, W)\n",
    "    j = grid[..., 1]\n",
    "    k = grid[..., 2]\n",
    "    i = tf.cast(i, 'float32')\n",
    "    j = tf.cast(j, 'float32')\n",
    "    k = tf.cast(k, 'float32')\n",
    "\n",
    "    # Scale i, j and k from [-1.0, 1.0] to [0, D], [0, H] and [0, W] respectively.\n",
    "    i = (i + 1.0) * 0.5 * tf.cast(nd-1, 'float32')\n",
    "    j = (j + 1.0) * 0.5 * tf.cast(nh-1, 'float32')\n",
    "    k = (k + 1.0) * 0.5 * tf.cast(nw-1, 'float32')\n",
    "\n",
    "    i_max = tf.cast(nd - 1, 'int32')\n",
    "    j_max = tf.cast(nh - 1, 'int32')\n",
    "    k_max = tf.cast(nw - 1, 'int32')\n",
    "    zero = tf.constant(0, 'int32')\n",
    "\n",
    "    # The value at (i, j, k) is a weighted average of the values at the\n",
    "    # eight nearest integer locations: (i0, j0, k0), (i0, j0, k1), (i0, j1, k0),\n",
    "    # (i0, j1, k1), (i1, j0, k0), (i1, j0, k1), (i1, j1, k0) and (i1, j1, k1)\n",
    "    # where i0 = floor(i), i1 = ceil(i).\n",
    "    i0 = tf.cast(tf.floor(i), 'int32')\n",
    "    i1 = i0 + 1\n",
    "    j0 = tf.cast(tf.floor(j), 'int32')\n",
    "    j1 = j0 + 1\n",
    "    k0 = tf.cast(tf.floor(k), 'int32')\n",
    "    k1 = k0 + 1\n",
    "\n",
    "    # Make sure indices are within the boundaries of the image.\n",
    "    i0 = tf.clip_by_value(i0, zero, i_max)\n",
    "    i1 = tf.clip_by_value(i1, zero, i_max)\n",
    "    j0 = tf.clip_by_value(j0, zero, j_max)\n",
    "    j1 = tf.clip_by_value(j1, zero, j_max)\n",
    "    k0 = tf.clip_by_value(k0, zero, k_max)\n",
    "    k1 = tf.clip_by_value(k1, zero, k_max)\n",
    "\n",
    "    # Collect indices of the four corners.\n",
    "    b = tf.ones_like(i0) * tf.reshape(tf.range(nb), [nb, 1, 1, 1])\n",
    "    idx_a = tf.stack([b, i1, j0, k0], axis=-1)  # all front-top-left corners\n",
    "    idx_b = tf.stack([b, i1, j1, k0], axis=-1)  # all front-bottom-left corners\n",
    "    idx_c = tf.stack([b, i1, j0, k1], axis=-1)  # all front-top-right corners\n",
    "    idx_d = tf.stack([b, i1, j1, k1], axis=-1)  # all front-bottom-right corners\n",
    "    idx_e = tf.stack([b, i0, j0, k0], axis=-1)  # all back-top-left corners\n",
    "    idx_f = tf.stack([b, i0, j1, k0], axis=-1)  # all back-bottom-left corners\n",
    "    idx_g = tf.stack([b, i0, j0, k1], axis=-1)  # all back-top-right corners\n",
    "    idx_h = tf.stack([b, i0, j1, k1], axis=-1)  # all back-bottom-right corners\n",
    "    # shape (N, D, H, W, 3)\n",
    "\n",
    "    # Collect values at the corners.\n",
    "    moving_a = tf.gather_nd(moving, idx_a)  # all front-top-left values\n",
    "    moving_b = tf.gather_nd(moving, idx_b)  # all front-bottom-left values\n",
    "    moving_c = tf.gather_nd(moving, idx_c)  # all front-top-right values\n",
    "    moving_d = tf.gather_nd(moving, idx_d)  # all front-bottom-right values\n",
    "    moving_e = tf.gather_nd(moving, idx_e)  # all back-top-left values\n",
    "    moving_f = tf.gather_nd(moving, idx_f)  # all back-bottom-left values\n",
    "    moving_g = tf.gather_nd(moving, idx_g)  # all back-top-right values\n",
    "    moving_h = tf.gather_nd(moving, idx_h)  # all back-bottom-right values\n",
    "    # shape (N, D, H, W, C)\n",
    "\n",
    "    i0_f = tf.cast(i0, 'float32')\n",
    "    i1_f = tf.cast(i1, 'float32')\n",
    "    j0_f = tf.cast(j0, 'float32')\n",
    "    j1_f = tf.cast(j1, 'float32')\n",
    "    k0_f = tf.cast(k0, 'float32')\n",
    "    k1_f = tf.cast(k1, 'float32')\n",
    "\n",
    "    # Calculate the weights.\n",
    "    wa = tf.expand_dims((i - i0_f) * (j1_f - j) * (k1_f - k), axis=-1)\n",
    "    wb = tf.expand_dims((i - i0_f) * (j - j0_f) * (k1_f - k), axis=-1)\n",
    "    wc = tf.expand_dims((i - i0_f) * (j1_f - j) * (k - k0_f), axis=-1)\n",
    "    wd = tf.expand_dims((i - i0_f) * (j - j0_f) * (k - k0_f), axis=-1)\n",
    "    we = tf.expand_dims((i1_f - i) * (j1_f - j) * (k1_f - k), axis=-1)\n",
    "    wf = tf.expand_dims((i1_f - i) * (j - j0_f) * (k1_f - k), axis=-1)\n",
    "    wg = tf.expand_dims((i1_f - i) * (j1_f - j) * (k - k0_f), axis=-1)\n",
    "    wh = tf.expand_dims((i1_f - i) * (j - j0_f) * (k - k0_f), axis=-1)\n",
    "\n",
    "    # Calculate the weighted sum.\n",
    "    moved = tf.add_n([wa * moving_a, wb * moving_b, wc * moving_c,\n",
    "                      wd * moving_d, we * moving_e, wf * moving_f,\n",
    "                      wg * moving_g, wh * moving_h])\n",
    "    return moved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelmorph1(input_shape=(128, 128, 1)):\n",
    "    moving = layers.Input(shape=input_shape, name='moving')\n",
    "    static = layers.Input(shape=input_shape, name='static')\n",
    "    x_in = layers.concatenate([static, moving], axis=-1)\n",
    "\n",
    "    # encoder\n",
    "    x1 = layers.Conv3D(16, kernel_size=3, strides=2, padding='same',\n",
    "                        kernel_initializer='he_normal')(x_in)\n",
    "    x1 = layers.LeakyReLU(alpha=0.2)(x1)  # 16\n",
    "\n",
    "    x2 = layers.Conv3D(32, kernel_size=3, strides=2, padding='same',\n",
    "                        kernel_initializer='he_normal')(x1)\n",
    "    x2 = layers.LeakyReLU(alpha=0.2)(x2)  # 8\n",
    "\n",
    "    x3 = layers.Conv3D(32, kernel_size=3, strides=2, padding='same',\n",
    "                        kernel_initializer='he_normal')(x2)\n",
    "    x3 = layers.LeakyReLU(alpha=0.2)(x3)  # 4\n",
    "\n",
    "    x4 = layers.Conv3D(32, kernel_size=3, strides=2, padding='same',\n",
    "                        kernel_initializer='he_normal')(x3)\n",
    "    x4 = layers.LeakyReLU(alpha=0.2)(x4)  # 2\n",
    "\n",
    "    # decoder [32, 32, 32, 32, 8, 8]\n",
    "    x = layers.Conv3D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x4)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.UpSampling3D(size=2)(x)  # 4\n",
    "    x = layers.concatenate([x, x3], axis=-1)  # 4\n",
    "\n",
    "    x = layers.Conv3D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.UpSampling3D(size=2)(x)  # 8\n",
    "    x = layers.concatenate([x, x2], axis=-1)  # 8\n",
    "\n",
    "    x = layers.Conv3D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.UpSampling3D(size=2)(x)  # 16\n",
    "    x = layers.concatenate([x, x1], axis=-1)  # 16\n",
    "\n",
    "    x = layers.Conv3D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Conv3D(8, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)  # 16\n",
    "\n",
    "    x = layers.UpSampling3D(size=2)(x)  # 32\n",
    "    x = layers.concatenate([x, x_in], axis=-1)\n",
    "    x = layers.Conv3D(8, kernel_size=3, strides=1, padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)  # 32\n",
    "\n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0,\n",
    "                                                            stddev=1e-5)\n",
    "    deformation = layers.Conv3D(3, kernel_size=3, strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=kernel_initializer)(x)\n",
    "\n",
    "    nb, nd, nh, nw, nc = tf.shape(deformation)\n",
    "\n",
    "    # Regular grid.\n",
    "    grid = regular_grid_3d(nd, nh, nw)  # shape (D, H, W, 2)\n",
    "    grid = tf.expand_dims(grid, axis=0)  # shape (1, D, H, W, 2)\n",
    "    multiples = tf.stack([nb, 1, 1, 1, 1])\n",
    "    grid = tf.tile(grid, multiples)\n",
    "\n",
    "    # Compute the new sampling grid.\n",
    "    grid_new = grid + deformation\n",
    "    grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
    "\n",
    "    # Sample the moving image using the new sampling grid.\n",
    "    moved = grid_sample_3d(moving, grid_new)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[static, moving],\n",
    "                            outputs=[moved, deformation], name='voxelmorph1')\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, moving, moving2, static, criterion, optimizer):\n",
    "    nb, nd, nh, nw, nc = tf.keras.backend.int_shape(moving)  # moving.shape\n",
    "\n",
    "    # # Repeat the static image along the batch dim.\n",
    "    # multiples = tf.constant([nb, 1, 1, 1, 1], tf.int32)\n",
    "    # static = tf.tile(static, multiples)\n",
    "\n",
    "    # Define the GradientTape context for automatic differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the deformation field\n",
    "        # inputs = tf.concat([moving, static], axis=-1)\n",
    "        moved, deformation = model({'moving': moving, 'static': static})\n",
    "\n",
    "        moved2, deformation2 = model({'moving': moving2, 'static': static})\n",
    "\n",
    "        # Compute the loss.\n",
    "        # loss = criterion(moved, static)\n",
    "        loss = -1 * criterion(static, moved) + 1 * gradient_loss(deformation) - 1 * criterion(static, moved2) + 1 * gradient_loss(deformation2) - 1 * criterion(moved, moved2)\n",
    "        \n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update the trainable parameters.\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, moving, moving2, static, criterion):\n",
    "    nb, nd, nh, nw, nc = tf.keras.backend.int_shape(moving)  # moving.shape\n",
    "\n",
    "    # Repeat the static image along the batch dim.\n",
    "    # multiples = tf.constant([nb, 1, 1, 1, 1], tf.int32)\n",
    "    # static = tf.tile(static, multiples)\n",
    "\n",
    "    # Get the deformation field.\n",
    "    # inputs = tf.concat([moving, static], axis=-1)\n",
    "    moved, deformation = model({'moving': moving, 'static': static}, training=False)\n",
    "    moved2, deformation2 = model({'moving': moving2, 'static': static}, training=False)\n",
    "\n",
    "    # Compute the loss.\n",
    "    # loss = criterion(moved, static)\n",
    "    loss = -1 * criterion(static, moved) + 1 * gradient_loss(deformation) - 1 * criterion(static, moved2) + 1 * gradient_loss(deformation2) - 1 * criterion(moved, moved2)\n",
    "    return loss\n",
    "\n",
    "def plot_images(model, moving, moving2, static):\n",
    "    nb, nd, nh, nw, nc = moving.shape\n",
    "\n",
    "    # Repeat the static image along the batch dim.\n",
    "    # multiples = tf.constant([nb, 1, 1, 1, 1], tf.int32)\n",
    "    # static = tf.tile(static, multiples)\n",
    "\n",
    "    moved, deformation = model({'moving': moving, 'static': static}, training=False)\n",
    "    moved2, deformation2 = model({'moving': moving2, 'static': static}, training=False)\n",
    "\n",
    "    tf.print(deformation.shape, tf.reduce_max(deformation), tf.reduce_min(deformation), tf.reduce_mean(deformation))\n",
    "    tf.print(deformation2.shape, tf.reduce_max(deformation2), tf.reduce_min(deformation2), tf.reduce_mean(deformation2))\n",
    "\n",
    "    deformation = deformation.numpy()\n",
    "    moved = moved.numpy().squeeze(axis=-1) * 255.0\n",
    "    # moved = moved.astype(np.uint8)[:,:,nh//2,...]\n",
    "    moving = moving.numpy().squeeze(axis=-1) * 255.0\n",
    "    # moving = moving.astype(np.uint8)[:,:,nh//2,...]\n",
    "    static = static.numpy().squeeze(axis=-1) * 255.0\n",
    "    # static = static.astype(np.uint8)[:,:,nh//2,...]\n",
    "\n",
    "    deformation2 = deformation2.numpy()\n",
    "    moved2 = moved2.numpy().squeeze(axis=-1) * 255.0\n",
    "    # moved = moved.astype(np.uint8)[:,:,nh//2,...]\n",
    "    moving2 = moving2.numpy().squeeze(axis=-1) * 255.0\n",
    "    # moving = moving.astype(np.uint8)[:,:,nh//2,...]\n",
    "\n",
    "    # # Plot images.\n",
    "    # fig = plt.figure(figsize=(3 * 1.7, nb * 1.7))\n",
    "    # titles_list = ['Static', 'Moved', 'Moving']\n",
    "    # images_list = [static, moved, moving]\n",
    "\n",
    "    # moved = moving\n",
    "    for i in range(nb):\n",
    "        # for j in range(3):\n",
    "        #     ax = fig.add_subplot(nb, 3, i * 3 + j + 1)\n",
    "        #     if i == 0:\n",
    "        #         ax.set_title(titles_list[j], fontsize=20)\n",
    "        #     ax.set_axis_off()\n",
    "        #     ax.imshow(images_list[j][i], cmap='gray')\n",
    "        regtools.overlay_slices(static[i], moved[i], None, 0,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_0.png\" % (i))\n",
    "        regtools.overlay_slices(static[i], moved[i], None, 1,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_1.png\" % (i))\n",
    "        regtools.overlay_slices(static[i], moved[i], None, 2,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_2.png\" % (i))\n",
    "        \n",
    "        regtools.overlay_slices(static[i], moved2[i], None, 0,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_0.png\" % (i))\n",
    "        regtools.overlay_slices(static[i], moved2[i], None, 1,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_1.png\" % (i))\n",
    "        regtools.overlay_slices(static[i], moved2[i], None, 2,\n",
    "                                \"%d Static\"%i, \"%d Transformed\"%i,\n",
    "                                \"%d_2.png\" % (i))\n",
    "        d = {'static': static[i], 'moved': moved[i], 'moving': moving[i], 'deformation': deformation[i],\n",
    "             'moved2': moved2[i], 'moving2': moving2[i], 'deformation2': deformation2[i]}\n",
    "        np.save('drive/My Drive/DIPY/t1_t2/results/sample%d.npy'%(i), d)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "def train_gen():\n",
    "    import random\n",
    "    random.shuffle(idx_list)\n",
    "    for file_idx in idx_list:\n",
    "        ap, _ = load_nifti(AP_list[file_idx])\n",
    "        ap = np.expand_dims(np.interp(ap, (ap.min(), ap.max()), (0, 1)), -1)\n",
    "        pa, _ = load_nifti(PA_list[file_idx])\n",
    "        pa = np.expand_dims(np.interp(pa, (pa.min(), pa.max()), (0, 1)), -1)\n",
    "        t1, _ = load_nifti(T1_list[file_idx])\n",
    "        t1 = np.interp(t1, (t1.min(), t1.max()), (0, 1))\n",
    "        yield ap, pa, t1\n",
    "\n",
    "def test_gen():\n",
    "    for file_idx in range(len(AP_list)-test_n, len(AP_list)):\n",
    "        ap, _ = load_nifti(AP_list[file_idx])\n",
    "        ap = np.expand_dims(np.interp(ap, (ap.min(), ap.max()), (0, 1)), -1)\n",
    "        pa, _ = load_nifti(PA_list[file_idx])\n",
    "        pa = np.expand_dims(np.interp(pa, (pa.min(), pa.max()), (0, 1)), -1)\n",
    "        t1, _ = load_nifti(T1_list[file_idx])\n",
    "        t1 = np.interp(t1, (t1.min(), t1.max()), (0, 1))\n",
    "        yield ap, pa, t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "    train_gen,\n",
    "    output_types=(tf.float32, tf.float32, tf.float32),\n",
    "    output_shapes=((128, 128, 128, 1), (128, 128, 128, 1), (128, 128, 128, 1))\n",
    "    )\n",
    "    train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_generator(\n",
    "    test_gen,\n",
    "    output_types=(tf.float32, tf.float32, tf.float32),\n",
    "    output_shapes=((128, 128, 128, 1), (128, 128, 128, 1), (128, 128, 128, 1))\n",
    "    )\n",
    "\n",
    "    test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    model = voxelmorph1(input_shape = (128, 128, 128, 1))\n",
    "    # model.load_weights('vxm_dense_brain_T1_3D_mse.h5')\n",
    "\n",
    "    # Select optimizer and loss function.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "    # criterion = ncc_new\n",
    "    bin_centers = np.linspace(0, 1, 32)\n",
    "    criterion = mutualInformation(bin_centers, sigma_ratio=0.5, max_clip=1, crop_background=False, local_mi=False, patch_size=12)\n",
    "\n",
    "    # Define the metrics to track training and testing losses.\n",
    "    m_train = tf.keras.metrics.Mean(name='loss_train')\n",
    "    m_test = tf.keras.metrics.Mean(name='loss_test')\n",
    "    \n",
    "    # Train and evaluate the model.\n",
    "    for epoch in range(args.epochs):\n",
    "        m_train.reset_states()\n",
    "        m_test.reset_states()\n",
    "\n",
    "        for moving, moving2, static in train_ds:\n",
    "            loss_train = train_step(model, moving, moving2, static, criterion,\n",
    "                                    optimizer)\n",
    "            m_train.update_state(loss_train)\n",
    "\n",
    "        for moving, moving2, static in test_ds:\n",
    "            loss_test = test_step(model, moving, moving2, static, criterion)\n",
    "            m_test.update_state(loss_test)\n",
    "\n",
    "        model.save_weights('%d.h5'%epoch)\n",
    "        tf.print('Epoch: %3d/%d\\tTrain Loss: %.6f\\tTest Loss: %.6f'\n",
    "              % (epoch + 1, args.epochs, m_train.result(), m_test.result()))\n",
    "    tf.print('\\n')\n",
    "\n",
    "    # Save the trained model.\n",
    "    if args.save_model:\n",
    "        model.save_weights('voxelmorph-weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 17:53:40.632777: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-10 17:53:44.508504: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd382c134a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-10 17:53:44.508525: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2023-12-10 17:53:44.511065: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702248824.574483  143256 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
        "Epoch:   1/100\tTrain Loss: 0.007069\tTest Loss: 0.001904\n",
        "Epoch:   2/100\tTrain Loss: 0.000998\tTest Loss: 0.000653\n",
        "Epoch:   3/100\tTrain Loss: 0.758103\tTest Loss: 0.019494\n",
        "Epoch:   4/100\tTrain Loss: 0.008574\tTest Loss: 0.005129\n",
        "Epoch:   5/100\tTrain Loss: 0.003580\tTest Loss: 0.002857\n",
        "Epoch:   6/100\tTrain Loss: 0.002631\tTest Loss: 0.002028\n",
        "Epoch:   7/100\tTrain Loss: 0.001535\tTest Loss: 0.000834\n",
        "Epoch:   8/100\tTrain Loss: 0.001465\tTest Loss: 0.000926\n",
        "Epoch:   9/100\tTrain Loss: 0.000899\tTest Loss: 0.000311\n",
        "Epoch:  10/100\tTrain Loss: 0.000265\tTest Loss: 0.000206\n",
        "Epoch:  11/100\tTrain Loss: 0.000201\tTest Loss: 0.000159\n",
        "Epoch:  12/100\tTrain Loss: 0.000147\tTest Loss: 0.000129\n",
        "Epoch:  13/100\tTrain Loss: 0.000124\tTest Loss: 0.000108\n",
        "Epoch:  14/100\tTrain Loss: 0.000104\tTest Loss: 0.000095\n",
        "Epoch:  15/100\tTrain Loss: 0.000092\tTest Loss: 0.000096\n",
        "Epoch:  16/100\tTrain Loss: 0.000083\tTest Loss: 0.000109\n",
        "Epoch:  17/100\tTrain Loss: 0.000077\tTest Loss: 0.000105\n",
        "Epoch:  18/100\tTrain Loss: 0.000073\tTest Loss: 0.000103\n",
        "Epoch:  19/100\tTrain Loss: 0.000071\tTest Loss: 0.000102\n",
        "Epoch:  20/100\tTrain Loss: 0.000069\tTest Loss: 0.000101\n",
        "Epoch:  21/100\tTrain Loss: 0.000068\tTest Loss: 0.000101\n",
        "Epoch:  22/100\tTrain Loss: 0.000067\tTest Loss: 0.000100\n",
        "Epoch:  23/100\tTrain Loss: 0.000066\tTest Loss: 0.000100\n",
        "Epoch:  24/100\tTrain Loss: 0.000065\tTest Loss: 0.000099\n",
        "Epoch:  25/100\tTrain Loss: 0.000065\tTest Loss: 0.000099\n",
        "Epoch:  26/100\tTrain Loss: 0.000064\tTest Loss: 0.000098\n",
        "Epoch:  27/100\tTrain Loss: 0.000064\tTest Loss: 0.000098\n",
        "Epoch:  28/100\tTrain Loss: 0.000063\tTest Loss: 0.000098\n",
        "Epoch:  29/100\tTrain Loss: 0.000063\tTest Loss: 0.000098\n",
        "Epoch:  30/100\tTrain Loss: 0.000062\tTest Loss: 0.000097\n",
        "Epoch:  31/100\tTrain Loss: 0.000062\tTest Loss: 0.000097\n",
        "Epoch:  32/100\tTrain Loss: 0.000062\tTest Loss: 0.000097\n",
        "Epoch:  33/100\tTrain Loss: 0.000061\tTest Loss: 0.000097\n",
        "Epoch:  34/100\tTrain Loss: 0.000061\tTest Loss: 0.000097\n",
        "Epoch:  35/100\tTrain Loss: 0.000061\tTest Loss: 0.000096\n",
        "Epoch:  36/100\tTrain Loss: 0.000060\tTest Loss: 0.000096\n",
        "Epoch:  37/100\tTrain Loss: 0.000060\tTest Loss: 0.000096\n",
        "Epoch:  38/100\tTrain Loss: 0.000060\tTest Loss: 0.000096\n",
        "Epoch:  39/100\tTrain Loss: 0.000060\tTest Loss: 0.000096\n",
        "Epoch:  40/100\tTrain Loss: 0.000059\tTest Loss: 0.000096\n",
        "Epoch:  41/100\tTrain Loss: 0.000059\tTest Loss: 0.000096\n",
        "Epoch:  42/100\tTrain Loss: 0.000059\tTest Loss: 0.000096\n",
        "Epoch:  43/100\tTrain Loss: 0.000059\tTest Loss: 0.000095\n",
        "Epoch:  44/100\tTrain Loss: 0.000059\tTest Loss: 0.000095\n",
        "Epoch:  45/100\tTrain Loss: 0.000059\tTest Loss: 0.000095\n",
        "Epoch:  46/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  47/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  48/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  49/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  50/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  51/100\tTrain Loss: 0.000058\tTest Loss: 0.000095\n",
        "Epoch:  52/100\tTrain Loss: 0.000057\tTest Loss: 0.000095\n",
        "Epoch:  53/100\tTrain Loss: 0.000057\tTest Loss: 0.000095\n",
        "Epoch:  54/100\tTrain Loss: 0.000057\tTest Loss: 0.000095\n"
        
     ]
    }
   ],
   "source": [
    "\"\"\"# Moved vs. Static\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    AP_list = []\n",
    "    PA_list = []\n",
    "    T1_list = []\n",
    "    idx_list = []\n",
    "    import os\n",
    "    for subj_n in os.listdir('data/AP'):\n",
    "        T1_list.append('data/T1/'+subj_n)\n",
    "        AP_list.append('data/AP/'+subj_n)\n",
    "        PA_list.append('data/PA/'+subj_n)\n",
    "\n",
    "    test_n = len(AP_list) // 10\n",
    "    idx_list = [i for i in range(len(AP_list) - test_n)]\n",
    "    # tf.print(AP_list[-test_n:])\n",
    "    class Args():\n",
    "        batch_size = BATCH_SIZE\n",
    "        epochs = 100\n",
    "        lr = 0.001\n",
    "        # label = 1  # which digit images to train on?\n",
    "        # num_samples = 5  # number of sample results to show\n",
    "        save_model = True\n",
    "    \n",
    "    args = Args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
