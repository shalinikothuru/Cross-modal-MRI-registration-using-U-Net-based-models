{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large kernel UNet to solve 3D image registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import dipy\n",
    "from dipy.io.image import load_nifti, save_nifti\n",
    "import nibabel as nib\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "import Data\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(pred1, truth1):\n",
    "    dice_35=np.zeros(35)\n",
    "    for k in range(1,36,1):\n",
    "        #print(k)\n",
    "        truth = truth1 == k\n",
    "        pred = pred1 == k\n",
    "        intersection = np.sum(pred[truth==1.0]) * 2.0\n",
    "        # print(intersection)\n",
    "        dice_35[k-1]=intersection / (np.sum(pred) + np.sum(truth))\n",
    "    return np.mean(dice_35)\n",
    "\n",
    "def save_checkpoint(state, save_dir, save_filename, max_model_num=10):\n",
    "    torch.save(state, save_dir + save_filename)\n",
    "    model_lists = natsorted(glob.glob(save_dir + '*'))\n",
    "    # print(model_lists)\n",
    "    while len(model_lists) > max_model_num:\n",
    "        os.remove(model_lists[0])\n",
    "        model_lists = natsorted(glob.glob(save_dir + '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_pair(data_path, filename1, filename2):\n",
    "    # Load images and labels\n",
    "    nim1 = nib.load(os.path.join(data_path, 'hyperdata', filename1, 'aligned_norm.nii.gz'))\n",
    "    image1 = nim1.get_data()\n",
    "    image1 = np.array(image1, dtype='float32')\n",
    "\n",
    "    nim2 = nib.load(os.path.join(data_path, 'hyperdata', filename2, 'aligned_norm.nii.gz'))\n",
    "    image2 = nim2.get_data()\n",
    "    image2 = np.array(image2, dtype='float32')\n",
    "        \n",
    "    nim5 = nib.load(os.path.join(data_path, 'hyperdata', filename1, 'aligned_seg35.nii.gz'))\n",
    "    image5 = nim5.get_data()\n",
    "    image5 = np.array(image5, dtype='float32')\n",
    "    \n",
    "    nim6 = nib.load(os.path.join(data_path, 'hyperdata', filename2, 'aligned_seg35.nii.gz'))\n",
    "    image6 = nim6.get_data()\n",
    "    image6 = np.array(image6, dtype='float32') \n",
    "    \n",
    "    image1 = np.reshape(image1, (1,) + image1.shape)\n",
    "    image2 = np.reshape(image2, (1,) + image2.shape)\n",
    "    image5 = np.reshape(image5, (1,) + image5.shape)\n",
    "    image6 = np.reshape(image6, (1,) + image6.shape)\n",
    "    return image1, image2, image5, image6\n",
    "\n",
    "class TrainDataset(Data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data_path, img_file=None, trainingset = 1):\n",
    "        'Initialization'\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.names = np.loadtxt(os.path.join(self.data_path, img_file),dtype='str')\n",
    "        if trainingset == 1:\n",
    "            self.filename = list(zip(self.names[:-1], self.names[1:]))\n",
    "            assert len(self.filename) == 200, \"# of images != 200.\"\n",
    "        elif trainingset == 2:\n",
    "            self.filename = list(zip(self.names[1:], self.names[:-1]))\n",
    "            assert len(self.filename) == 200, \"# of images != 200.\"\n",
    "        elif trainingset == 3:\n",
    "            self.zip_filename_1 = list(zip(self.names[:-1], self.names[1:]))\n",
    "            self.zip_filename_2 = list(zip(self.names[1:], self.names[:-1]))\n",
    "            self.filename = self.zip_filename_1 + self.zip_filename_2\n",
    "            assert len(self.filename) == 400, \"# of images != 400.\"\n",
    "        elif trainingset == 4:\n",
    "            self.filename = list(itertools.permutations(self.names, 2))\n",
    "            # print(len(self.names))\n",
    "            # print(len(self.filename))\n",
    "            assert len(self.filename) == 154842, \"# of images != 154842.\"\n",
    "        \n",
    "        else:\n",
    "             assert 0==1, print('TrainDataset Invalid!')\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.filename)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        mov_img, fix_img, mov_lab, fix_lab = load_train_pair(self.data_path, self.filename[index][0], self.filename[index][1])\n",
    "        return  mov_img, fix_img, mov_lab, fix_lab\n",
    "\n",
    "class ValidationDataset(Data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data_path, transform=None):\n",
    "        'Initialization'\n",
    "        super(ValidationDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.filename = pd.read_csv(os.path.join(data_path,'pairs_val.csv')).values\n",
    "        #print(self.filename)\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.filename)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        assert len(self.filename)==19, \"# of images != 19.\"\n",
    "        # Select sample\n",
    "        img_A, img_B, label_A, label_B = load_validation_pair(self.data_path, self.filename[index][0], self.filename[index][1])\n",
    "        return self.filename[index][0],self.filename[index][1], img_A, img_B, label_A, label_B\n",
    "def load_validation_pair(data_path, fixed, moving):\n",
    "    # Load images and labels\n",
    "    nim1 = nib.load(os.path.join(data_path, 'hyperdata',  'OASIS_OAS1_0'+str(fixed)+'_MR1', 'aligned_norm.nii.gz'))\n",
    "    image1 = nim1.get_data()[:, :, :]\n",
    "    image1 = np.array(image1, dtype='float32')\n",
    "\n",
    "    nim2 = nib.load(os.path.join(data_path, 'hyperdata',  'OASIS_OAS1_0'+str(moving)+'_MR1', 'aligned_norm.nii.gz'))\n",
    "    image2 = nim2.get_data()[:, :, :]\n",
    "    image2 = np.array(image2, dtype='float32')\n",
    "    \n",
    "    nim3 = nib.load(os.path.join(data_path, 'hyperdata',  'OASIS_OAS1_0'+str(fixed)+'_MR1', 'aligned_seg35.nii.gz'))\n",
    "    image3 = nim3.get_data()[:, :, :]\n",
    "    image3 = np.array(image3, dtype='float32')\n",
    "\n",
    "    nim4 = nib.load(os.path.join(data_path, 'hyperdata',  'OASIS_OAS1_0'+str(moving)+'_MR1', 'aligned_seg35.nii.gz'))\n",
    "    image4 = nim4.get_data()[:, :, :]\n",
    "    image4 = np.array(image4, dtype='float32')\n",
    "    #preprocessing\n",
    "    image1 = np.reshape(image1, (1,) + image1.shape)\n",
    "    image2 = np.reshape(image2, (1,) + image2.shape)\n",
    "    image3 = np.reshape(image3, (1,) + image3.shape)\n",
    "    image4 = np.reshape(image4, (1,) + image4.shape)\n",
    "    return image1, image2,image3,image4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large kernel UNet model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on LKU-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeKernel_encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding=2, bias=False, batchnorm=False):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.bias = bias\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        super(LargeKernel_encoder, self).__init__()\n",
    "        \n",
    "        self.layer_regularKernel = self.encoder_LargeKernel_encoder(self.in_channels, self.out_channels, kernel_size = 3, stride=1, padding=1, bias=self.bias, batchnorm = self.batchnorm)\n",
    "        self.layer_largeKernel = self.encoder_LargeKernel_encoder(self.in_channels, self.out_channels, kernel_size = self.kernel_size, stride=self.stride, padding=self.padding, bias=self.bias, batchnorm = self.batchnorm)\n",
    "        self.layer_oneKernel = self.encoder_LargeKernel_encoder(self.in_channels, self.out_channels, kernel_size = 1, stride=1, padding=0, bias=self.bias, batchnorm = self.batchnorm)\n",
    "        self.layer_nonlinearity = nn.PReLU()\n",
    "    def encoder_LargeKernel_encoder(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False, batchnorm=False):\n",
    "        if batchnorm:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm3d(out_channels))\n",
    "        else:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias))\n",
    "        return layer\n",
    "    def forward(self, inputs):\n",
    "        regularKernel = self.layer_regularKernel(inputs)\n",
    "        largeKernel = self.layer_largeKernel(inputs)\n",
    "        oneKernel = self.layer_oneKernel(inputs)\n",
    "        outputs = regularKernel + largeKernel + oneKernel + inputs\n",
    "\n",
    "        return self.layer_nonlinearity(outputs)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channel, n_classes, start_channel):\n",
    "        self.in_channel = in_channel\n",
    "        self.n_classes = n_classes\n",
    "        self.start_channel = start_channel\n",
    "        bias_opt = True\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "        self.eninput = self.encoder(self.in_channel, self.start_channel, bias=bias_opt)\n",
    "        self.ec1 = self.encoder(self.start_channel, self.start_channel, bias=bias_opt)\n",
    "        self.ec2 = self.encoder(self.start_channel, self.start_channel * 2, stride=2, bias=bias_opt)\n",
    "        self.ec3 = LargeKernel_encoder(self.start_channel * 2, self.start_channel * 2, kernel_size=5, stride=1, padding=2, bias=bias_opt)\n",
    "        self.ec4 = self.encoder(self.start_channel * 2, self.start_channel * 4, stride=2, bias=bias_opt)\n",
    "        self.ec5 = LargeKernel_encoder(self.start_channel * 4, self.start_channel * 4, kernel_size=5, stride=1, padding=2, bias=bias_opt)\n",
    "        self.ec6 = self.encoder(self.start_channel * 4, self.start_channel * 8, stride=2, bias=bias_opt)\n",
    "        self.ec7 = LargeKernel_encoder(self.start_channel * 8, self.start_channel * 8, kernel_size=5, stride=1, padding=2, bias=bias_opt)\n",
    "        self.ec8 = self.encoder(self.start_channel * 8, self.start_channel * 8, stride=2, bias=bias_opt)\n",
    "        self.ec9 = LargeKernel_encoder(self.start_channel * 8, self.start_channel * 8, kernel_size=5, stride=1, padding=2, bias=bias_opt)\n",
    "\n",
    "        self.dc1 = self.encoder(self.start_channel * 8 + self.start_channel * 8, self.start_channel * 8, kernel_size=3,\n",
    "                                stride=1, bias=bias_opt)\n",
    "        self.dc2 = self.encoder(self.start_channel * 8, self.start_channel * 4, kernel_size=3, stride=1, bias=bias_opt)\n",
    "        self.dc3 = self.encoder(self.start_channel * 4 + self.start_channel * 4, self.start_channel * 4, kernel_size=3,\n",
    "                                stride=1, bias=bias_opt)\n",
    "        self.dc4 = self.encoder(self.start_channel * 4, self.start_channel * 2, kernel_size=3, stride=1, bias=bias_opt)\n",
    "        self.dc5 = self.encoder(self.start_channel * 2 + self.start_channel * 2, self.start_channel * 4, kernel_size=3,\n",
    "                                stride=1, bias=bias_opt)\n",
    "        self.dc6 = self.encoder(self.start_channel * 4, self.start_channel * 2, kernel_size=3, stride=1, bias=bias_opt)\n",
    "        self.dc7 = self.encoder(self.start_channel * 2 + self.start_channel * 1, self.start_channel * 2, kernel_size=3,\n",
    "                                stride=1, bias=bias_opt)\n",
    "        self.dc8 = self.encoder(self.start_channel * 2, self.start_channel * 2, kernel_size=3, stride=1, bias=bias_opt)\n",
    "        self.dc9 = self.outputs(self.start_channel * 2, self.n_classes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.up1 = self.decoder(self.start_channel * 8, self.start_channel * 8)\n",
    "        self.up2 = self.decoder(self.start_channel * 4, self.start_channel * 4)\n",
    "        self.up3 = self.decoder(self.start_channel * 2, self.start_channel * 2)\n",
    "        self.up4 = self.decoder(self.start_channel * 2, self.start_channel * 2)\n",
    "\n",
    "    def encoder(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n",
    "                bias=False, batchnorm=False):\n",
    "        if batchnorm:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.PReLU())\n",
    "        else:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.PReLU())\n",
    "        return layer\n",
    "\n",
    "    def decoder(self, in_channels, out_channels, kernel_size=2, stride=2, padding=0,\n",
    "                output_padding=0, bias=True):\n",
    "        layer = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                               padding=padding, output_padding=output_padding, bias=bias),\n",
    "            nn.PReLU())\n",
    "        return layer\n",
    "\n",
    "    def outputs(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n",
    "                bias=False, batchnorm=False):\n",
    "        if batchnorm:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.BatchNorm3d(out_channels),\n",
    "                nn.Tanh())\n",
    "        else:\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "                nn.Softsign())\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_in = torch.cat((x, y), 1)\n",
    "        e0 = self.eninput(x_in)\n",
    "        e0 = self.ec1(e0)\n",
    "\n",
    "        e1 = self.ec2(e0)\n",
    "        e1 = self.ec3(e1)\n",
    "\n",
    "        e2 = self.ec4(e1)\n",
    "        e2 = self.ec5(e2)\n",
    "\n",
    "        e3 = self.ec6(e2)\n",
    "        e3 = self.ec7(e3)\n",
    "\n",
    "        e4 = self.ec8(e3)\n",
    "        e4 = self.ec9(e4)\n",
    "\n",
    "        d0 = torch.cat((self.up1(e4), e3), 1)\n",
    "\n",
    "        d0 = self.dc1(d0)\n",
    "        d0 = self.dc2(d0)\n",
    "\n",
    "        d1 = torch.cat((self.up2(d0), e2), 1)\n",
    "\n",
    "        d1 = self.dc3(d1)\n",
    "        d1 = self.dc4(d1)\n",
    "\n",
    "        d2 = torch.cat((self.up3(d1), e1), 1)\n",
    "\n",
    "        d2 = self.dc5(d2)\n",
    "        d2 = self.dc6(d2)\n",
    "\n",
    "        d3 = torch.cat((self.up4(d2), e0), 1)\n",
    "        d3 = self.dc7(d3)\n",
    "        d3 = self.dc8(d3)\n",
    "\n",
    "        f_xy = self.dc9(d3)\n",
    "        #f_yx = self.dc10(d3)\n",
    "\n",
    "        return f_xy#, f_yx\n",
    "\n",
    "class SpatialTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialTransform, self).__init__()\n",
    "    def forward(self, mov_image, flow, mod = 'bilinear'):\n",
    "        d2, h2, w2 = mov_image.shape[-3:]\n",
    "        grid_d, grid_h, grid_w = torch.meshgrid([torch.linspace(-1, 1, d2), torch.linspace(-1, 1, h2), torch.linspace(-1, 1, w2)])\n",
    "        grid_h = grid_h.cuda().float()\n",
    "        grid_d = grid_d.cuda().float()\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_d = nn.Parameter(grid_d, requires_grad=False)\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "        flow_d = flow[:,:,:,:,0]\n",
    "        flow_h = flow[:,:,:,:,1]\n",
    "        flow_w = flow[:,:,:,:,2]\n",
    "        #Softsign\n",
    "        #disp_d = (grid_d + (flow_d * 2 / d2)).squeeze(1)\n",
    "        #disp_h = (grid_h + (flow_h * 2 / h2)).squeeze(1)\n",
    "        #disp_w = (grid_w + (flow_w * 2 / w2)).squeeze(1)\n",
    "        \n",
    "        # Remove Channel Dimension\n",
    "        disp_d = (grid_d + (flow_d)).squeeze(1)\n",
    "        disp_h = (grid_h + (flow_h)).squeeze(1)\n",
    "        disp_w = (grid_w + (flow_w)).squeeze(1)\n",
    "\n",
    "        sample_grid = torch.stack((disp_w, disp_h, disp_d), 4)  # shape (N, D, H, W, 3)\n",
    "        warped = torch.nn.functional.grid_sample(mov_image, sample_grid, mode = mod, align_corners = True)\n",
    "        \n",
    "        return warped#, sample_grid\n",
    "class SpatialTransform_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialTransform_1, self).__init__()\n",
    "    def forward(self, mov_image, flow, mod = 'bilinear'):\n",
    "        d2, h2, w2 = mov_image.shape[-3:]\n",
    "        grid_d, grid_h, grid_w = torch.meshgrid([torch.linspace(-1, 1, d2), torch.linspace(-1, 1, h2), torch.linspace(-1, 1, w2)])\n",
    "        grid_h = grid_h.cuda().float()\n",
    "        grid_d = grid_d.cuda().float()\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_d = nn.Parameter(grid_d, requires_grad=False)\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "        flow_d = flow[:,:,:,:,0]\n",
    "        flow_h = flow[:,:,:,:,1]\n",
    "        flow_w = flow[:,:,:,:,2]\n",
    "\n",
    "        disp_d = (grid_d + (flow_d)).squeeze(1)\n",
    "        disp_h = (grid_h + (flow_h)).squeeze(1)\n",
    "        disp_w = (grid_w + (flow_w)).squeeze(1)\n",
    "\n",
    "        sample_grid = torch.stack((disp_w, disp_h, disp_d), 4)  # shape (N, D, H, W, 3)\n",
    "        warped = torch.nn.functional.grid_sample(mov_image, sample_grid, mode = mod, align_corners = True)\n",
    "        \n",
    "        return warped, sample_grid\n",
    "\n",
    "class DiffeomorphicTransform(nn.Module):\n",
    "    def __init__(self, time_step=7):\n",
    "        super(DiffeomorphicTransform, self).__init__()\n",
    "        self.time_step = time_step\n",
    "\n",
    "    def forward(self, flow):\n",
    "    \n",
    "        # print(flow.shape)\n",
    "        d2, h2, w2 = flow.shape[-3:]\n",
    "        grid_d, grid_h, grid_w = torch.meshgrid([torch.linspace(-1, 1, d2), torch.linspace(-1, 1, h2), torch.linspace(-1, 1, w2)])\n",
    "        grid_h = grid_h.cuda().float()\n",
    "        grid_d = grid_d.cuda().float()\n",
    "        grid_w = grid_w.cuda().float()\n",
    "        grid_d = nn.Parameter(grid_d, requires_grad=False)\n",
    "        grid_w = nn.Parameter(grid_w, requires_grad=False)\n",
    "        grid_h = nn.Parameter(grid_h, requires_grad=False)\n",
    "        flow = flow / (2 ** self.time_step)\n",
    "        \n",
    "        \n",
    "        for i in range(self.time_step):\n",
    "            flow_d = flow[:,0,:,:,:]\n",
    "            flow_h = flow[:,1,:,:,:]\n",
    "            flow_w = flow[:,2,:,:,:]\n",
    "            disp_d = (grid_d + flow_d).squeeze(1)\n",
    "            disp_h = (grid_h + flow_h).squeeze(1)\n",
    "            disp_w = (grid_w + flow_w).squeeze(1)\n",
    "            \n",
    "            deformation = torch.stack((disp_w, disp_h, disp_d), 4)  # shape (N, D, H, W, 3)\n",
    "\n",
    "            flow = flow + torch.nn.functional.grid_sample(flow, deformation, mode='bilinear', padding_mode=\"border\", align_corners = True)\n",
    "\n",
    "        \n",
    "        return flow\n",
    "\n",
    "\n",
    "class CompositionTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompositionTransform, self).__init__()\n",
    "\n",
    "    def forward(self, flow_1, flow_2, sample_grid, range_flow):\n",
    "        size_tensor = sample_grid.size()\n",
    "        grid = sample_grid + (flow_1.permute(0,2,3,4,1) * range_flow)\n",
    "        grid[:, :, :, :, 0] = (grid[:, :, :, :, 0] - ((size_tensor[3] - 1) / 2)) / (size_tensor[3] - 1) * 2\n",
    "        grid[:, :, :, :, 1] = (grid[:, :, :, :, 1] - ((size_tensor[2] - 1) / 2)) / (size_tensor[2] - 1) * 2\n",
    "        grid[:, :, :, :, 2] = (grid[:, :, :, :, 2] - ((size_tensor[1] - 1) / 2)) / (size_tensor[1] - 1) * 2\n",
    "        compos_flow = F.grid_sample(flow_2, grid, mode='bilinear', align_corners = True) + flow_1\n",
    "        return compos_flow\n",
    "\n",
    "\n",
    "def smoothloss(y_pred):\n",
    "    #print('smoothloss y_pred.shape    ',y_pred.shape)\n",
    "    #[N,3,D,H,W]\n",
    "    d2, h2, w2 = y_pred.shape[-3:]\n",
    "    dy = torch.abs(y_pred[:,:,1:, :, :] - y_pred[:,:, :-1, :, :]) / 2 * d2\n",
    "    dx = torch.abs(y_pred[:,:,:, 1:, :] - y_pred[:,:, :, :-1, :]) / 2 * h2\n",
    "    dz = torch.abs(y_pred[:,:,:, :, 1:] - y_pred[:,:, :, :, :-1]) / 2 * w2\n",
    "    return (torch.mean(dx * dx)+torch.mean(dy*dy)+torch.mean(dz*dz))/3.0\n",
    "\n",
    "\n",
    "def JacboianDet(y_pred, sample_grid):\n",
    "    J = y_pred + sample_grid\n",
    "    dy = J[:, 1:, :-1, :-1, :] - J[:, :-1, :-1, :-1, :]\n",
    "    dx = J[:, :-1, 1:, :-1, :] - J[:, :-1, :-1, :-1, :]\n",
    "    dz = J[:, :-1, :-1, 1:, :] - J[:, :-1, :-1, :-1, :]\n",
    "\n",
    "    Jdet0 = dx[:,:,:,:,0] * (dy[:,:,:,:,1] * dz[:,:,:,:,2] - dy[:,:,:,:,2] * dz[:,:,:,:,1])\n",
    "    Jdet1 = dx[:,:,:,:,1] * (dy[:,:,:,:,0] * dz[:,:,:,:,2] - dy[:,:,:,:,2] * dz[:,:,:,:,0])\n",
    "    Jdet2 = dx[:,:,:,:,2] * (dy[:,:,:,:,0] * dz[:,:,:,:,1] - dy[:,:,:,:,1] * dz[:,:,:,:,0])\n",
    "\n",
    "    Jdet = Jdet0 - Jdet1 + Jdet2\n",
    "\n",
    "    return Jdet\n",
    "\n",
    "\n",
    "def neg_Jdet_loss(y_pred, sample_grid):\n",
    "    neg_Jdet = -1.0 * JacboianDet(y_pred, sample_grid)\n",
    "    selected_neg_Jdet = F.relu(neg_Jdet)\n",
    "\n",
    "    return torch.mean(selected_neg_Jdet)\n",
    "\n",
    "\n",
    "def magnitude_loss(flow_1, flow_2):\n",
    "    num_ele = torch.numel(flow_1)\n",
    "    flow_1_mag = torch.sum(torch.abs(flow_1))\n",
    "    flow_2_mag = torch.sum(torch.abs(flow_2))\n",
    "\n",
    "    diff = (torch.abs(flow_1_mag - flow_2_mag))/num_ele\n",
    "\n",
    "    return diff\n",
    "\n",
    "class NCC(torch.nn.Module):\n",
    "    def __init__(self, win=9, eps=1e-5):\n",
    "        super(NCC, self).__init__()\n",
    "        self.win_raw = win\n",
    "        self.eps = eps\n",
    "        self.win = win\n",
    "\n",
    "    def forward(self, I, J):\n",
    "        ndims = 3\n",
    "        win_size = self.win_raw\n",
    "        self.win = [self.win_raw] * ndims\n",
    "\n",
    "        weight_win_size = self.win_raw\n",
    "        weight = torch.ones((1, 1, weight_win_size, weight_win_size, weight_win_size), device=I.device, requires_grad=False)\n",
    "        conv_fn = F.conv3d\n",
    "\n",
    "        I2 = I*I\n",
    "        J2 = J*J\n",
    "        IJ = I*J\n",
    "\n",
    "\n",
    "        I_sum = conv_fn(I, weight, padding=int(win_size/2))\n",
    "        J_sum = conv_fn(J, weight, padding=int(win_size/2))\n",
    "        I2_sum = conv_fn(I2, weight, padding=int(win_size/2))\n",
    "        J2_sum = conv_fn(J2, weight, padding=int(win_size/2))\n",
    "        IJ_sum = conv_fn(IJ, weight, padding=int(win_size/2))\n",
    "\n",
    "        win_size = np.prod(self.win)\n",
    "        u_I = I_sum/win_size\n",
    "        u_J = J_sum/win_size\n",
    "\n",
    "        cross = IJ_sum - u_J*I_sum - u_I*J_sum + u_I*u_J*win_size\n",
    "        I_var = I2_sum - 2 * u_I * I_sum + u_I*u_I*win_size\n",
    "        J_var = J2_sum - 2 * u_J * J_sum + u_J*u_J*win_size\n",
    "\n",
    "        cc = cross * cross / (I_var * J_var + self.eps)\n",
    "\n",
    "        return -1.0 * torch.mean(cc)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, num_class=36):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        y_true = nn.functional.one_hot(y_true, num_classes=self.num_class)\n",
    "        y_true = torch.squeeze(y_true, 1)\n",
    "        y_true = y_true.permute(0, 4, 1, 2, 3).contiguous()\n",
    "        intersection = y_pred * y_true\n",
    "        intersection = intersection.sum(dim=[2, 3, 4])\n",
    "        union = torch.pow(y_pred, 2).sum(dim=[2, 3, 4]) + torch.pow(y_true, 2).sum(dim=[2, 3, 4])\n",
    "        dsc = (2.*intersection) / (union + 1e-5)\n",
    "        dsc = (1-torch.mean(dsc))\n",
    "        return dsc\n",
    "\n",
    "class MSE:\n",
    "\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "class SAD:\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return torch.mean(torch.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    use_cuda = True\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = UNet(2, 3, 32).cuda()\n",
    "\n",
    "    loss_similarity = NCC()\n",
    "    loss_smooth = smoothloss\n",
    "    loss_dice = DiceLoss()\n",
    "    iterations = 309685\n",
    "    smooth = 0.01\n",
    "    lr = 0.0001\n",
    "    n_checkpoint = 1000\n",
    "\n",
    "    transform = SpatialTransform().cuda()\n",
    "    diff_transform = DiffeomorphicTransform(time_step=7).cuda()\n",
    "\n",
    "    for param in transform.parameters():\n",
    "        param.requires_grad = False\n",
    "        param.volatile = True\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    lossall = np.zeros((4, iterations))\n",
    "    trainingset = \"ts/\"\n",
    "    bs = 2\n",
    "    train_set = TrainDataset(\"OASIS\",img_file='imglist.txt', trainingset = trainingset)\n",
    "    training_generator = Data.DataLoader(dataset=train_set, batch_size=bs, shuffle=True, num_workers=4)\n",
    "    test_set = ValidationDataset(opt.datapath)#,img_file='val_list.txt')\n",
    "    test_generator = Data.DataLoader(dataset=test_set, batch_size=bs, shuffle=False, num_workers=2)\n",
    "    model_dir = \"trained-largekernel\"\n",
    "    \n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    \n",
    "    \n",
    "    step = 1\n",
    "\n",
    "    while step <= iterations:\n",
    "        for mov_img, fix_img, mov_lab, fix_lab in training_generator:\n",
    "\n",
    "            fix_img = fix_img.cuda().float()\n",
    "\n",
    "            mov_img = mov_img.cuda().float()\n",
    "\n",
    "            fix_lab = fix_lab.cuda().float()\n",
    "\n",
    "            mov_lab = mov_lab.cuda().float()\n",
    "            \n",
    "            X_Seg = nn.functional.one_hot(mov_lab.long(), num_classes=36)\n",
    "            X_Seg = X_Seg.squeeze(1).permute(0, 4, 1, 2, 3)\n",
    "            f_xy = model(mov_img, fix_img)\n",
    "\n",
    "            \n",
    "            warped_mov = transform(mov_img, f_xy.permute(0, 2, 3, 4, 1))\n",
    "            X_Y_Seg = transform(X_Seg.float(), f_xy.permute(0, 2, 3, 4, 1))\n",
    "           \n",
    "            loss1 = loss_similarity(fix_img, warped_mov)\n",
    "            loss2 = loss_smooth(f_xy)\n",
    "            loss3 = loss_dice(X_Y_Seg, fix_lab.long())\n",
    "            mask_labda = 0.1\n",
    "            loss = loss1 + smooth * loss2 + mask_labda * loss3\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lossall[:,step] = np.array([loss.item(), loss1.item(), loss2.item(), loss3.item()])\n",
    "\n",
    "            if (step % n_checkpoint == 0):\n",
    "                with torch.no_grad():\n",
    "                    Dices_Validation = []\n",
    "                    for __, __, mov_img, fix_img, mov_lab, fix_lab in test_generator:\n",
    "                        model.eval()\n",
    "                        V_xy = model(mov_img.float().to(device), fix_img.float().to(device))\n",
    "\n",
    "                        warped_mov_lab = transform(mov_lab.float().to(device), V_xy.permute(0, 2, 3, 4, 1), mod = 'nearest')\n",
    "                        for bs_index in range(bs):\n",
    "                            dice_bs = dice(warped_mov_lab[bs_index,...].data.cpu().numpy().copy(),fix_lab[bs_index,...].data.cpu().numpy().copy())\n",
    "                            Dices_Validation.append(dice_bs)\n",
    "                    modelname = 'DiceVal_{:.4f}_Step_{:09d}.pth'.format(np.mean(Dices_Validation), step)\n",
    "                    csv_dice = np.mean(Dices_Validation)\n",
    "                    save_checkpoint(model.state_dict(), model_dir, modelname)\n",
    "                    np.save(model_dir + 'Loss.npy', lossall)\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "            if step > 309685:\n",
    "                break\n",
    "    # np.save(model_dir + '/Loss.npy', lossall)\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
